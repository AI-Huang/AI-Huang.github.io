<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Academic</title><link>https://AI-Huang.github.io/</link><atom:link href="https://AI-Huang.github.io/index.xml" rel="self" type="application/rss+xml"/><description>Academic</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Thu, 01 Dec 2022 17:28:00 +0800</lastBuildDate><image><url>https://AI-Huang.github.io/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png</url><title>Academic</title><link>https://AI-Huang.github.io/</link></image><item><title>Replication of SENet</title><link>https://AI-Huang.github.io/research/replication_senet/</link><pubDate>Thu, 01 Dec 2022 17:28:00 +0800</pubDate><guid>https://AI-Huang.github.io/research/replication_senet/</guid><description>&lt;p>This is a replication of the work SENet (&lt;a href="https://arxiv.org/abs/1709.01507" target="_blank" rel="noopener">J. Hu, et al., Squeeze-and-Excitation Networks&lt;/a>). My codes:&lt;/p>
&lt;ul>
&lt;li>Implement the SENet module;&lt;/li>
&lt;li>Apply the SENet module to the ResNet;&lt;/li>
&lt;li>Train the ResNet with SENet on CIFAR-10;&lt;/li>
&lt;li>Use the re-trained benchmark results of ResNet on CIFAR-10 for comparative evaluation.&lt;/li>
&lt;/ul>
&lt;p>For statistical validation, each group of experiment has been run for 5 times.&lt;/p>
&lt;p>J. Hu et al.&amp;rsquo;s experiment group uses pad, crop and flip augmentation, while I use random tranlation augmentation. Both groups use standard deviation normalization. Number in the brackets of test accuracy are the difference from the ResNet backbone to the SE-ResNet counterpart.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model&lt;/th>
&lt;th>Author&lt;/th>
&lt;th>best test accuracy&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>ResNet20v1_CIFAR10&lt;/td>
&lt;td>Kan&lt;/td>
&lt;td>91.30%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ResNet32v1_CIFAR10&lt;/td>
&lt;td>Kan&lt;/td>
&lt;td>92.16%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ResNet110v1_CIFAR10&lt;/td>
&lt;td>Kan&lt;/td>
&lt;td>92.10%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ResNet164v1_CIFAR10&lt;/td>
&lt;td>Kan&lt;/td>
&lt;td>91.74%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>SE-ResNet20 (γ=16)&lt;/td>
&lt;td>Kan&lt;/td>
&lt;td>91.70% (+0.4)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>SE-ResNet32 (γ=16)&lt;/td>
&lt;td>Kan&lt;/td>
&lt;td>92.44% (+0.28)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>SE-ResNet110 (γ=16)&lt;/td>
&lt;td>Kan&lt;/td>
&lt;td>86.56% (-5.54)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>SE-ResNet164 (γ=16)&lt;/td>
&lt;td>Kan&lt;/td>
&lt;td>55.25% (-36.49)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>SE-ResNet110 (γ=16)&lt;/td>
&lt;td>J. Hu et al.&lt;/td>
&lt;td>5.21 (94.79%) (+1.16)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>SE-ResNet164 (γ=16)&lt;/td>
&lt;td>J. Hu et al.&lt;/td>
&lt;td>4.39 (95.61%) (+1.07)&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>Experiments on SE-ResNet20 and SE-ResNet32 show that SE module works well on enhancing the backbone network. But due to the backbone&amp;rsquo;s performance limitation (e.g., ResNet20), such enhancement is relatively limited. Training on SE-ResNet110 and SE-ResNet164 doesn&amp;rsquo;t converge, I&amp;rsquo;m still figuring out why.&lt;/p></description></item><item><title>Benchmark of ResNet on CIFAR-10</title><link>https://AI-Huang.github.io/research/benchmark_resnet/</link><pubDate>Tue, 29 Nov 2022 13:19:00 +0800</pubDate><guid>https://AI-Huang.github.io/research/benchmark_resnet/</guid><description>&lt;p>This is a &lt;strong>TensorFlow&lt;/strong> replication of experiments on CIFAR-10 mentioned in ResNet (&lt;a href="https://arxiv.org/abs/1512.03385" target="_blank" rel="noopener">K. He, et al., Deep Residual Learning for Image Recognition&lt;/a>). My codes:&lt;/p>
&lt;ul>
&lt;li>Adapts Keras&amp;rsquo;s example codes of ResNet for CIFAR-10 (note that this is a simpler version specially designed for CIFAR-10);&lt;/li>
&lt;li>Apply the SENet module to the ResNet;&lt;/li>
&lt;li>Re-train the ResNet w/o SENet on CIFAR-10 for benchmark evaluation.&lt;/li>
&lt;/ul>
&lt;p>For statistical validation, each group of experiment has been run for 5 times.&lt;/p>
&lt;p>Firstly, I try to reproduce the results with totally the same &lt;a href="https://github.com/AI-Huang/CV_Playground/blob/master/models/tf_fn/resnet_cifar10.py" target="_blank" rel="noopener">model codes&lt;/a> given by Keras. The batch size is set to 32. The optimizer is Adam with an initial learning rate 0.001. The validation split is 0, and the whole train set is used as training data. The data augmentation could be found at &lt;a href="https://github.com/AI-Huang/CV_Playground/blob/master/data_loaders/tf_fn/data_generator.py" target="_blank" rel="noopener">keras_augmentation&lt;/a>. The leanring scheduler could be found at &lt;a href="https://github.com/AI-Huang/CV_Playground/blob/master/models/tf_fn/optim_utils.py" target="_blank" rel="noopener">keras_lr_scheduler&lt;/a>. Our results outperform Keras&amp;rsquo;s on &lt;code>ResNet44v1_CIFAR10&lt;/code>, but go worse a little on other models.&lt;/p>
&lt;p>ResNet20 keras_augmentation Adam 0.001 0 keras_lr_scheduler 0.9183&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model&lt;/th>
&lt;th>Author&lt;/th>
&lt;th>best test accuracy&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>ResNet20v1_CIFAR10&lt;/td>
&lt;td>Keras&lt;/td>
&lt;td>0.9216&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ResNet20v1_CIFAR10&lt;/td>
&lt;td>Kan&lt;/td>
&lt;td>0.9183&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ResNet32v1_CIFAR10&lt;/td>
&lt;td>Keras&lt;/td>
&lt;td>0.9246&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ResNet32v1_CIFAR10&lt;/td>
&lt;td>Kan&lt;/td>
&lt;td>0.9227&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ResNet44v1_CIFAR10&lt;/td>
&lt;td>Keras&lt;/td>
&lt;td>0.9250&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ResNet44v1_CIFAR10&lt;/td>
&lt;td>Kan&lt;/td>
&lt;td>0.9252&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ResNet56v1_CIFAR10&lt;/td>
&lt;td>Keras&lt;/td>
&lt;td>0.9271&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ResNet56v1_CIFAR10&lt;/td>
&lt;td>Kan&lt;/td>
&lt;td>0.9236&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ResNet110v1_CIFAR10&lt;/td>
&lt;td>Keras&lt;/td>
&lt;td>0.9265&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ResNet110v1_CIFAR10&lt;/td>
&lt;td>Kan&lt;/td>
&lt;td>0.9260&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>Later, I follow the same configuration by K. He, et al., and use standard normalization. Random translation (padding, and then crop with a horizonal flip) is also applied. The validation split changes from 0 to 0.1 thus results in a 45k/5k train/val split. We follow the same optimizer settings. We use the SGD optimizer with an initial learning rate of 0.1, a momentum of 0.9, a weight decay of 0.0001 and finally a mini-batch size of 128.
&lt;code>cifar10_scheduler&lt;/code> is applied. In original paper, learning rate is reduced by a factor 0.1 on some a step such as the 32k and 48k step. Here we convert these step indices into epoch indices by equation:
$$ steps_per_epoch = \lceil \frac{45000}{batch_size} \rceil , $$
$$ epoch_to_reduce_lr = \lceil \frac{32000|48000}{steps_per_epoch} \rceil . $$
Thus we have an adapted schedule:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>steps (batchsize 128)&lt;/th>
&lt;th>epoch&lt;/th>
&lt;th>LR (SGD)&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>32k&lt;/td>
&lt;td>1~91&lt;/td>
&lt;td>0.1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>48k&lt;/td>
&lt;td>92~137&lt;/td>
&lt;td>0.01&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>64k&lt;/td>
&lt;td>137~182&lt;/td>
&lt;td>0.001&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>The results are listed as the table below:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model&lt;/th>
&lt;th>pre-processing&lt;/th>
&lt;th>data_augmentation&lt;/th>
&lt;th>Author&lt;/th>
&lt;th>best test accuracy&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>ResNet20v1_CIFAR10&lt;/td>
&lt;td>subtract_mean&lt;/td>
&lt;td>pad_crop_flip&lt;/td>
&lt;td>He et al.&lt;/td>
&lt;td>91.25%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ResNet20v1_CIFAR10&lt;/td>
&lt;td>std_norm&lt;/td>
&lt;td>random_translation&lt;/td>
&lt;td>Kan&lt;/td>
&lt;td>91.30%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ResNet32v1_CIFAR10&lt;/td>
&lt;td>subtract_mean&lt;/td>
&lt;td>pad_crop_flip&lt;/td>
&lt;td>He et al.&lt;/td>
&lt;td>92.49%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ResNet32v1_CIFAR10&lt;/td>
&lt;td>std_norm&lt;/td>
&lt;td>random_translation&lt;/td>
&lt;td>Kan&lt;/td>
&lt;td>92.16%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ResNet44v1_CIFAR10&lt;/td>
&lt;td>subtract_mean&lt;/td>
&lt;td>pad_crop_flip&lt;/td>
&lt;td>He et al.&lt;/td>
&lt;td>92.83%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ResNet44v1_CIFAR10&lt;/td>
&lt;td>std_norm&lt;/td>
&lt;td>random_translation&lt;/td>
&lt;td>Kan&lt;/td>
&lt;td>N/A&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ResNet56v1_CIFAR10&lt;/td>
&lt;td>subtract_mean&lt;/td>
&lt;td>pad_crop_flip&lt;/td>
&lt;td>He et al.&lt;/td>
&lt;td>93.03%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ResNet56v1_CIFAR10&lt;/td>
&lt;td>std_norm&lt;/td>
&lt;td>random_translation&lt;/td>
&lt;td>Kan&lt;/td>
&lt;td>N/A&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ResNet110v1_CIFAR10&lt;/td>
&lt;td>subtract_mean&lt;/td>
&lt;td>pad_crop_flip&lt;/td>
&lt;td>He et al.&lt;/td>
&lt;td>93.39+-.16%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ResNet110v1_CIFAR10&lt;/td>
&lt;td>std_norm&lt;/td>
&lt;td>random_translation&lt;/td>
&lt;td>Kan&lt;/td>
&lt;td>0.9210&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ResNet164v1_CIFAR10&lt;/td>
&lt;td>subtract_mean&lt;/td>
&lt;td>pad_crop_flip&lt;/td>
&lt;td>He et al.&lt;/td>
&lt;td>N/A&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ResNet164v1_CIFAR10&lt;/td>
&lt;td>std_norm&lt;/td>
&lt;td>random_translation&lt;/td>
&lt;td>Kan&lt;/td>
&lt;td>0.9174&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>The main difference between &lt;code>random_translation&lt;/code> and &lt;code>pad_crop_flip&lt;/code> is that the former supply with a new &lt;code>fill_mode='nearest'&lt;/code>.
Among the model that both He et al. and me have the results, our results only outperform He&amp;rsquo;s on ResNet20v1_CIFAR10, and are not as good as He claims on ResNet32v1_CIFAR10 and ResNet110v1_CIFAR10.
Models listed below are further used as benchmarks for SENet counterpart:&lt;/p>
&lt;ul>
&lt;li>ResNet20v1_CIFAR10&lt;/li>
&lt;li>ResNet32v1_CIFAR10&lt;/li>
&lt;li>ResNet110v1_CIFAR10&lt;/li>
&lt;li>ResNet164v1_CIFAR10&lt;/li>
&lt;/ul></description></item><item><title>Present at the Workshop on Applied Machine Learning Methods for Time Series Forecasting</title><link>https://AI-Huang.github.io/events/amlts22-cikm-workshop/</link><pubDate>Fri, 21 Oct 2022 08:00:00 -0400</pubDate><guid>https://AI-Huang.github.io/events/amlts22-cikm-workshop/</guid><description>&lt;p>We present our work &amp;ldquo;Kan Huang, Kai Zhang and Ming Liu. GreenEyes: An Air Quality Evaluating Model based on WaveNet&amp;rdquo; at the Workshop on Applied Machine Learning Methods for Time Series Forecasting (&lt;a href="https://www.cikm2022.org/workshops" target="_blank" rel="noopener">AMLTS'22&lt;/a>).&lt;/p>
&lt;figure id="figure-amlts22-presentation">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="AMLTS&amp;#39;22 Presentation" srcset="
/events/amlts22-cikm-workshop/AMLTS%20Presentation_hu7117e7612380ba3649a3613133ba0483_366149_29c96f27ec4f6667249a1fb9336377f2.webp 400w,
/events/amlts22-cikm-workshop/AMLTS%20Presentation_hu7117e7612380ba3649a3613133ba0483_366149_5655776d8e8332dcb69ab417701c354c.webp 760w,
/events/amlts22-cikm-workshop/AMLTS%20Presentation_hu7117e7612380ba3649a3613133ba0483_366149_1200x1200_fit_q75_h2_lanczos.webp 1200w"
src="https://AI-Huang.github.io/events/amlts22-cikm-workshop/AMLTS%20Presentation_hu7117e7612380ba3649a3613133ba0483_366149_29c96f27ec4f6667249a1fb9336377f2.webp"
width="760"
height="320"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption data-pre="Figure&amp;nbsp;" data-post=":&amp;nbsp;" class="numbered">
AMLTS'22 Presentation
&lt;/figcaption>&lt;/figure></description></item><item><title>GreenEyes: An Air Quality Evaluating Model based on WaveNet</title><link>https://AI-Huang.github.io/research/greeneyes/</link><pubDate>Sun, 18 Sep 2022 21:14:15 +0800</pubDate><guid>https://AI-Huang.github.io/research/greeneyes/</guid><description>&lt;p>Accepted by &lt;em>&lt;a href="https://AI-Huang.github.io/publication/amlts2022/">AMLTS 2022&lt;/a>&lt;/em>.&lt;/p>
&lt;p>Presentation available at &lt;a href="https://AI-Huang.github.io/events/amlts22-cikm-workshop/">AMLTS 2022 Workshop&lt;/a>.&lt;/p></description></item><item><title>1 paper has been accepted by AMLTS 2022.</title><link>https://AI-Huang.github.io/events/amlts2022/</link><pubDate>Sat, 17 Sep 2022 20:18:01 +0800</pubDate><guid>https://AI-Huang.github.io/events/amlts2022/</guid><description>&lt;p>Our work &amp;ldquo;Kan Huang, Kai Zhang and Ming Liu. GreenEyes: An Air Quality Evaluating Model based on WaveNet&amp;rdquo; has been accepted by Applied Machine Learning Methods for Time Series Forecasting (&lt;a href="https://amlts.github.io/amlts2022/#Papers" target="_blank" rel="noopener">AMLTS&lt;/a>) 2022. AMLTS is the conjunction workshop of the top conference ACM International Conference on Information and Knowledge Management (&lt;a href="https://www.cikm2022.org/" target="_blank" rel="noopener">CIKM&lt;/a>) 🤗.&lt;/p></description></item><item><title>RMNIST/N: Train MNIST dataset with only TEN samples</title><link>https://AI-Huang.github.io/research/rmnist_n/</link><pubDate>Tue, 09 Aug 2022 22:02:00 +0800</pubDate><guid>https://AI-Huang.github.io/research/rmnist_n/</guid><description>&lt;p>&lt;a href="https://cognitivemedium.com/rmnist" target="_blank" rel="noopener">RMNIST/N&lt;/a> is a dataset that reduces MNIST with N examples for each digit class. In this way, RMNIST/1 has 1 training example for each digit, for a total of only 10 training examples. This article presents how a digits recognizer can be trained on only ten samples from the whole MNIST dataset. Data augmentation is used during the training. Ablation study is also made.&lt;/p></description></item><item><title>GreenEyes: An Air Quality Evaluating Model based on WaveNet</title><link>https://AI-Huang.github.io/publication/amlts2022/</link><pubDate>Fri, 01 Jul 2022 00:00:00 +0000</pubDate><guid>https://AI-Huang.github.io/publication/amlts2022/</guid><description/></item><item><title>Course Lecturer, Greedy Technology</title><link>https://AI-Huang.github.io/teaching/course_lecturer/</link><pubDate>Mon, 13 Sep 2021 14:23:00 +0800</pubDate><guid>https://AI-Huang.github.io/teaching/course_lecturer/</guid><description>&lt;p>&lt;a target="_blank" href="https://uai.greedyai.com/">Greedy Technology&lt;/a> is a company based in Beijing. It focuses on teaching students machine learning knowledge and related technical applications, with a professional educating team. I assisted this company as a Course Lecturer during Oct. 2021 to Apr. 2022, and designed one basic tutorial and five application courses.&lt;/p>
&lt;p>Tutorial:&lt;/p>
&lt;ul>
&lt;li>Chatbot based on RNN and PyTorch&lt;/li>
&lt;/ul>
&lt;p>Application Courses:&lt;/p>
&lt;ul>
&lt;li>Object Detection based on Faster RCNN&lt;/li>
&lt;li>Image Style Transfer based on Neural Style&lt;/li>
&lt;li>Image Captioning based on RNN,
&lt;i class="fas fa-code pr-1 fa-fw">&lt;/i> &lt;a target="_blank" href="https://github.com/AI-Huang/ImageCaptioning">Code&lt;/a>&lt;/li>
&lt;li>Earthquake Prediction based on WaveNet,
&lt;i class="fas fa-code pr-1 fa-fw">&lt;/i> &lt;a target="_blank" href="https://github.com/AI-Huang/LANL_Earthquake_Prediction">Code&lt;/a>&lt;/li>
&lt;li>Logo Synthesis based on iWGAN&lt;/li>
&lt;/ul></description></item><item><title>Image Compression with Flow Models</title><link>https://AI-Huang.github.io/research/flow_model/</link><pubDate>Tue, 18 May 2021 16:58:00 +0800</pubDate><guid>https://AI-Huang.github.io/research/flow_model/</guid><description>&lt;ul>
&lt;li>Found new approaches of using flow models to do compression tasks for image‐like data. Wrote processing and feature extracting functions for 80K+ target data. Tested VQ‐VAE and basic Real NVP model to build unsupervising learning baselines;&lt;/li>
&lt;li>Based on open source codes, wrote hundreds lines of extra codes to restore Real NVP Compression model introduced by the SOTA papers. Did experiments with different feature extractors on the 80K+ 2D data; it turned out that the permutations of pixels will influence the results;&lt;/li>
&lt;li>Found Glow model’s learning potential; transplanted it from TensorFlow to PyTorch;&lt;/li>
&lt;li>Conclusion: flow models have great potential on distribution transformation, but better permutated data is also necessary.&lt;/li>
&lt;/ul></description></item><item><title>A TensorFlow Implementation of AdderNet</title><link>https://AI-Huang.github.io/research/addernet_tensorflow/</link><pubDate>Fri, 29 Jan 2021 17:00:00 +0800</pubDate><guid>https://AI-Huang.github.io/research/addernet_tensorflow/</guid><description>&lt;p>This is a A TensorFlow Implementation of AdderNet. The original paper, AdderNet (&lt;a href="https://arxiv.org/abs/1912.13200" target="_blank" rel="noopener">H. Chen, et al., AdderNet: Do We Really Need Multiplications in Deep Learning?&lt;/a>), is implemented with PyTorch. Here we provide a TensorFlow alternative for training and evaluating the Adder layer operator and related network backbones.&lt;/p>
&lt;p>The codes is available (click the &amp;ldquo;Codes&amp;rdquo; button).&lt;/p></description></item><item><title>AdderNet Presentation</title><link>https://AI-Huang.github.io/research/addernet_presentation/</link><pubDate>Fri, 29 Jan 2021 17:00:00 +0800</pubDate><guid>https://AI-Huang.github.io/research/addernet_presentation/</guid><description>&lt;p>This is a paper digest sharing presentation in the reading group. The featured paper is AdderNet (&lt;a href="https://arxiv.org/abs/1912.13200" target="_blank" rel="noopener">H. Chen, et al., AdderNet: Do We Really Need Multiplications in Deep Learning?&lt;/a>).&lt;/p>
&lt;p>Presentation slides is available (click the &amp;ldquo;Slides&amp;rdquo; button).&lt;/p></description></item><item><title>Semi-conductor Image Classification</title><link>https://AI-Huang.github.io/research/semi_conductor/</link><pubDate>Thu, 30 Jan 2020 12:25:00 +0800</pubDate><guid>https://AI-Huang.github.io/research/semi_conductor/</guid><description>&lt;ul>
&lt;li>Helped &lt;a href="https://www.math.hkust.edu.hk/people/faculty/profile/yuany/" target="_blank" rel="noopener">Prof. YAO Yuan&lt;/a> and the Nexperia company with the Nexperia semi‐conductor classification problem;&lt;/li>
&lt;li>Distinguished broken chips from normal chips by using deep learning and image classification methods;&lt;/li>
&lt;li>Compared the difference of perform between different models, e.g., ResNet20, ResNet56;&lt;/li>
&lt;li>Tried transferred learning methods. It turned out that a ResNet56 model transferred from a pretrained ResNet20 model could perform better, than simply training a ResNet56 model directly.&lt;/li>
&lt;/ul></description></item><item><title>Seq2Seq Chatbot</title><link>https://AI-Huang.github.io/research/seq2seq_chatbot/</link><pubDate>Thu, 30 May 2019 20:20:00 +0800</pubDate><guid>https://AI-Huang.github.io/research/seq2seq_chatbot/</guid><description>&lt;p>This program:&lt;/p>
&lt;ul>
&lt;li>Builds a Seq2Seq model and learn on the text dataset;&lt;/li>
&lt;li>Uses beam search algorithm to generate output;&lt;/li>
&lt;li>Supplys a dialogue robot backend for the Chatbot.&lt;/li>
&lt;/ul>
&lt;p>&lt;a href="https://AI-Huang.github.io/projects/seq2seq_qq_chatbot/">Other parts of the Chatbot&lt;/a>.&lt;/p></description></item><item><title>Course Designer</title><link>https://AI-Huang.github.io/teaching/course_designer/</link><pubDate>Thu, 25 Apr 2019 20:49:00 +0800</pubDate><guid>https://AI-Huang.github.io/teaching/course_designer/</guid><description>&lt;p>Many people are interested about start-ups on education industries in Hong Kong. During Apr. 2019 to May. 2019, I served as Course Designer to help several entrepreneurship founders design lectures and courses to introduce knowledge in the university to common high school students.&lt;/p>
&lt;p>I designed a lecture which introduces the prelimilary background of ECE.
&lt;i class="fas fa-download pr-1 fa-fw">&lt;/i> Download my &lt;a target="_blank" href="https://docs.google.com/presentation/d/1WF-Nrc0zMbXW6NHoZSGQavefwHPqnoOU/edit?usp=sharing&amp;ouid=115182618927559529877&amp;rtpof=true&amp;sd=true">PPT on Introduction to ECE&lt;/a>.&lt;/p>
&lt;p>In the meanwhile, I have designed several tiny embedded projects to enhance the understanding of the background of ECE:&lt;/p>
&lt;ul>
&lt;li>Running Water Lights;&lt;/li>
&lt;li>Electronic Voter.&lt;/li>
&lt;/ul></description></item><item><title>Seq2Seq QQ Chatbot</title><link>https://AI-Huang.github.io/projects/seq2seq_qq_chatbot/</link><pubDate>Fri, 15 Feb 2019 16:02:00 +0800</pubDate><guid>https://AI-Huang.github.io/projects/seq2seq_qq_chatbot/</guid><description>&lt;ul>
&lt;li>Built a chatbot on the QQ platform with the Coolq Framework. Implemented the following functions&lt;/li>
&lt;li>chatbot: built an application interface for the Seq2Seq model, gave the chatbot the ability to response for users’ input;&lt;/li>
&lt;li>dictionary command: using Web crawler, the chatbot can return word’s meaning;&lt;/li>
&lt;li>translate command: using Google translate library, the chatbot can retrive the translation for users’ input;&lt;/li>
&lt;li>sticker command: the chatbot could query the SQL database to find stickers that users demand;&lt;/li>
&lt;li>Other features: group members’ same words repeater; weather querying; banning user’s chat; search image by image.&lt;/li>
&lt;/ul>
&lt;p>&lt;a href="https://AI-Huang.github.io/research/seq2seq_chatbot/">The Chatbot&amp;rsquo;s dialogue robot model&lt;/a>.&lt;/p></description></item><item><title>Teaching Assistant</title><link>https://AI-Huang.github.io/teaching/teaching_assistant/</link><pubDate>Wed, 01 Feb 2017 00:00:00 +0800</pubDate><guid>https://AI-Huang.github.io/teaching/teaching_assistant/</guid><description>&lt;p>I have served as Teaching Assistant for one year and a half at HKUST. Courses I served as TA:&lt;/p>
&lt;ul>
&lt;li>ELEC2600: Stochastic Processes&lt;/li>
&lt;li>ELEC6910R: Robotic Perception and Learning&lt;/li>
&lt;/ul>
&lt;p>ELEC6910R is a very interesting course about deep learning and robotics taught by my supervisor &lt;a target="_blank" href="https://ram-lab.com/people/">Prof. Ming LIU&lt;/a>.&lt;/p>
&lt;p>
&lt;i class="fas fa-download pr-1 fa-fw">&lt;/i> Download the &lt;a target="_blank" href="https://drive.google.com/drive/folders/1ozymE1rmwsNDzfHjA8pMABRnCE6OKDaJ?usp=sharing">course materials here&lt;/a>.&lt;/p></description></item><item><title>Seed Cup 2014</title><link>https://AI-Huang.github.io/events/seedcup2014/</link><pubDate>Tue, 18 Nov 2014 20:43:47 +0800</pubDate><guid>https://AI-Huang.github.io/events/seedcup2014/</guid><description>
&lt;figure id="figure-seed-cup-2014-champion">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Seed Cup 2014 Champion" srcset="
/events/seedcup2014/SeedCup2014_%E5%86%A0%E5%86%9B_hu62d390962baf5698b58614aba09d6b3c_112215_13fb707535c8fd3397254189be3e078e.webp 400w,
/events/seedcup2014/SeedCup2014_%E5%86%A0%E5%86%9B_hu62d390962baf5698b58614aba09d6b3c_112215_42e01cade92429f37c390b122f40494d.webp 760w,
/events/seedcup2014/SeedCup2014_%E5%86%A0%E5%86%9B_hu62d390962baf5698b58614aba09d6b3c_112215_1200x1200_fit_q75_h2_lanczos.webp 1200w"
src="https://AI-Huang.github.io/events/seedcup2014/SeedCup2014_%E5%86%A0%E5%86%9B_hu62d390962baf5698b58614aba09d6b3c_112215_13fb707535c8fd3397254189be3e078e.webp"
width="750"
height="500"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption data-pre="Figure&amp;nbsp;" data-post=":&amp;nbsp;" class="numbered">
Seed Cup 2014 Champion
&lt;/figcaption>&lt;/figure></description></item><item><title/><link>https://AI-Huang.github.io/admin/config.yml</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://AI-Huang.github.io/admin/config.yml</guid><description/></item></channel></rss>