[{"authors":null,"categories":null,"content":"Kelley Kan HUANG is an engineer of the Hong Kong Applied Science and Technology Research Institute (ASTRI). Before that he graduated from the Hong Kong University of Science and Technology at the Department of Electronics and Computer Engineering in August, 2021 and obtained his M.Phil. certificate. Kan received his Bachelor’s degree from Huazhong University of Science and Technology in Electronic and Information Engineering in 2016, with a GPA ranking 5 out of 171. After that, he enrolled and participated in entrepreneurial activities for 2 years in Hong Kong.\nKan’s research interests include Neural Networks, Machine Learning, Representation Learning and Natural Language Processing. He also has some AIoT and Robotics experience.\nBesides skills related to machine learning, Kan is also familiar with some engineering technics such as HTML, Django, Web Crawler and Bash Scripts.\nDownload my resumé.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"Kelley Kan HUANG is an engineer of the Hong Kong Applied Science and Technology Research Institute (ASTRI). Before that he graduated from the Hong Kong University of Science and Technology at the Department of Electronics and Computer Engineering in August, 2021 and obtained his M.","tags":null,"title":"Kelley Kan HUANG","type":"authors"},{"authors":["Kan Huang"],"categories":[],"content":"This is a TensorFlow replication of experiments on CIFAR-10 mentioned in ResNet (K. He, et al., Deep Residual Learning for Image Recognition). My codes:\nAdapts Keras’s example codes of ResNet for CIFAR-10 (note that this is a simpler version specially designed for CIFAR-10); Apply the SENet module to the ResNet; Re-train the ResNet w/o SENet on CIFAR-10 for benchmark evaluation. For statistical validation, each group of experiment has been run for 5 times.\nFirstly, I try to reproduce the results with totally the same model codes given by Keras. The batch size is set to 32. The optimizer is Adam with an initial learning rate 0.001. The validation split is 0, and the whole train set is used as training data. The data augmentation could be found at keras_augmentation. The leanring scheduler could be found at keras_lr_scheduler. Our results outperform Keras’s on ResNet44v1_CIFAR10, but go worse a little on other models.\nResNet20 keras_augmentation Adam 0.001 0 keras_lr_scheduler 0.9183\nModel Author best test accuracy ResNet20v1_CIFAR10 Keras 0.9216 ResNet20v1_CIFAR10 Kan 0.9183 ResNet32v1_CIFAR10 Keras 0.9246 ResNet32v1_CIFAR10 Kan 0.9227 ResNet44v1_CIFAR10 Keras 0.9250 ResNet44v1_CIFAR10 Kan 0.9252 ResNet56v1_CIFAR10 Keras 0.9271 ResNet56v1_CIFAR10 Kan 0.9236 ResNet110v1_CIFAR10 Keras 0.9265 ResNet110v1_CIFAR10 Kan 0.9260 Later, I follow the same configuration by K. He, et al., and use standard normalization. Random translation (padding, and then crop with a horizonal flip) is also applied. The validation split changes from 0 to 0.1 thus results in a 45k/5k train/val split. We follow the same optimizer settings. We use the SGD optimizer with an initial learning rate of 0.1, a momentum of 0.9, a weight decay of 0.0001 and finally a mini-batch size of 128. cifar10_scheduler is applied. In original paper, learning rate is reduced by a factor 0.1 on some a step such as the 32k and 48k step. Here we convert these step indices into epoch indices by equation: $$ steps_per_epoch = \\lceil \\frac{45000}{batch_size} \\rceil , $$ $$ epoch_to_reduce_lr = \\lceil \\frac{32000|48000}{steps_per_epoch} \\rceil . $$ Thus we have an adapted schedule:\nsteps (batchsize 128) epoch LR (SGD) 32k 1~91 0.1 48k 92~137 0.01 64k 137~182 0.001 The results are listed as the table below:\nModel pre-processing data_augmentation Author best test accuracy ResNet20v1_CIFAR10 subtract_mean pad_crop_flip He et al. 91.25% ResNet20v1_CIFAR10 std_norm random_translation Kan 91.30% ResNet32v1_CIFAR10 subtract_mean pad_crop_flip He et al. 92.49% ResNet32v1_CIFAR10 std_norm random_translation Kan 92.16% ResNet44v1_CIFAR10 subtract_mean pad_crop_flip He et al. 92.83% ResNet44v1_CIFAR10 std_norm random_translation Kan N/A ResNet56v1_CIFAR10 subtract_mean pad_crop_flip He et al. 93.03% ResNet56v1_CIFAR10 std_norm random_translation Kan N/A ResNet110v1_CIFAR10 subtract_mean pad_crop_flip He et al. 93.39+-.16% ResNet110v1_CIFAR10 std_norm random_translation Kan 0.9210 ResNet164v1_CIFAR10 subtract_mean pad_crop_flip He et al. N/A ResNet164v1_CIFAR10 std_norm random_translation Kan 0.9174 The main difference between random_translation and pad_crop_flip is that the former supply with a new fill_mode=\u0026#39;nearest\u0026#39;. Among the model that both He et al. and me have the results, our results only outperform He’s on ResNet20v1_CIFAR10, and are not as good as He claims on ResNet32v1_CIFAR10 and ResNet110v1_CIFAR10. Models listed below are further used as benchmarks for SENet counterpart:\nResNet20v1_CIFAR10 ResNet32v1_CIFAR10 ResNet110v1_CIFAR10 ResNet164v1_CIFAR10 ","date":1669699140,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1669699140,"objectID":"249209bb4cfc8d9a4ac803efb01bc38d","permalink":"https://AI-Huang.github.io/research/benchmark_resnet/","publishdate":"2022-11-29T13:19:00+08:00","relpermalink":"/research/benchmark_resnet/","section":"research","summary":"This is a TensorFlow replication of experiments on CIFAR-10 mentioned in ResNet (K. He, et al., Deep Residual Learning for Image Recognition). My codes:\nAdapts Keras’s example codes of ResNet for CIFAR-10 (note that this is a simpler version specially designed for CIFAR-10); Apply the SENet module to the ResNet; Re-train the ResNet w/o SENet on CIFAR-10 for benchmark evaluation.","tags":["Benchmark","ResNet","Computer Vision"],"title":"Benchmark of ResNet on CIFAR-10","type":"research"},{"authors":["Kan Huang"],"categories":[],"content":"This is a replication of the work SENet (J. Hu, et al., Squeeze-and-Excitation Networks). My codes:\nImplement the SENet module; Apply the SENet module to the ResNet; Train the ResNet with SENet on CIFAR-10; Use the re-trained benchmark results of ResNet on CIFAR-10 for comparative evaluation. For statistical validation, each group of experiment has been run for 5 times.\nJ. Hu et al.’s experiment group uses pad, crop and flip augmentation, while I use random tranlation augmentation. Both groups use standard deviation normalization. Number in the brackets of test accuracy are the difference from the ResNet backbone to the SE-ResNet counterpart.\nModel Author best test accuracy ResNet20v1_CIFAR10 Kan 91.30% ResNet32v1_CIFAR10 Kan 92.16% ResNet110v1_CIFAR10 Kan 92.10% ResNet164v1_CIFAR10 Kan 91.74% SE-ResNet20 (γ=16) Kan 91.70% (+0.4) SE-ResNet32 (γ=16) Kan 92.44% (+0.28) SE-ResNet110 (γ=16) Kan 86.56% (-5.54) SE-ResNet164 (γ=16) Kan 55.25% (-36.49) SE-ResNet110 (γ=16) J. Hu et al. 5.21 (94.79%) (+1.16) SE-ResNet164 (γ=16) J. Hu et al. 4.39 (95.61%) (+1.07) Experiments on SE-ResNet20 and SE-ResNet32 show that SE module works well on enhancing the backbone network. But due to the backbone’s performance limitation (e.g., ResNet20), such enhancement is relatively limited. Training on SE-ResNet110 and SE-ResNet164 doesn’t converge, I’m still figuring out why.\n","date":1669107600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1669107600,"objectID":"675b354425fa516bdb714c0606e92837","permalink":"https://AI-Huang.github.io/research/replication_senet/","publishdate":"2022-11-22T17:00:00+08:00","relpermalink":"/research/replication_senet/","section":"research","summary":"This is a replication of the work SENet (J. Hu, et al., Squeeze-and-Excitation Networks). My codes:\nImplement the SENet module; Apply the SENet module to the ResNet; Train the ResNet with SENet on CIFAR-10; Use the re-trained benchmark results of ResNet on CIFAR-10 for comparative evaluation.","tags":["Replication","SENet","Computer Vision"],"title":"Replication of SENet","type":"research"},{"authors":["Kan Huang","Kai Zhang","Ming Liu"],"categories":null,"content":"We present our work “Kan Huang, Kai Zhang and Ming Liu. GreenEyes: An Air Quality Evaluating Model based on WaveNet” at the Workshop on Applied Machine Learning Methods for Time Series Forecasting (AMLTS\u0026#39;22).\nAMLTS\u0026#39;22 Presentation ","date":1666353600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1666353600,"objectID":"a32df8c8245e39ceed73db0dab1e5bee","permalink":"https://AI-Huang.github.io/events/amlts22-cikm-workshop/","publishdate":"2022-10-21T08:00:00-04:00","relpermalink":"/events/amlts22-cikm-workshop/","section":"events","summary":"We present our work at the Workshop on Applied Machine Learning Methods for Time Series Forecasting","tags":["deep learning","time series forecasting"],"title":"Present at the Workshop on Applied Machine Learning Methods for Time Series Forecasting","type":"events"},{"authors":[],"categories":[],"content":"Accepted by AMLTS 2022.\nPresentation available at AMLTS 2022 Workshop.\n","date":1663506855,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1663506855,"objectID":"bbe5d365cc82bc9e8785c533cfb847d0","permalink":"https://AI-Huang.github.io/research/greeneyes/","publishdate":"2022-09-18T21:14:15+08:00","relpermalink":"/research/greeneyes/","section":"research","summary":"Accepted by AMLTS 2022.\nPresentation available at AMLTS 2022 Workshop.","tags":[],"title":"GreenEyes: An Air Quality Evaluating Model based on WaveNet","type":"research"},{"authors":["Kan Huang","Kai Zhang","Ming Liu"],"categories":null,"content":"Our work “Kan Huang, Kai Zhang and Ming Liu. GreenEyes: An Air Quality Evaluating Model based on WaveNet” has been accepted by Applied Machine Learning Methods for Time Series Forecasting (AMLTS) 2022. AMLTS is the conjunction workshop of the top conference ACM International Conference on Information and Knowledge Management (CIKM) 🤗.\n","date":1663417081,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1663417081,"objectID":"81463e0a33b2001d22ac5bce910fc44b","permalink":"https://AI-Huang.github.io/events/amlts2022/","publishdate":"2022-09-17T20:18:01+08:00","relpermalink":"/events/amlts2022/","section":"events","summary":"Our work on time series forecasting has been accepted by AMLTS 2022","tags":["deep learning","time series forecasting"],"title":"1 paper has been accepted by AMLTS 2022.","type":"events"},{"authors":[],"categories":["Deep Learning","Data Augmentation"],"content":"RMNIST/N is a dataset that reduces MNIST with N examples for each digit class. In this way, RMNIST/1 has 1 training example for each digit, for a total of only 10 training examples. This article presents how a digits recognizer can be trained on only ten samples from the whole MNIST dataset. Data augmentation is used during the training. Ablation study is also made.\n","date":1660053720,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1660053720,"objectID":"c73775c2ebad32cce779fb880aef441f","permalink":"https://AI-Huang.github.io/research/rmnist_n/","publishdate":"2022-08-09T22:02:00+08:00","relpermalink":"/research/rmnist_n/","section":"research","summary":"RMNIST/N is a dataset that reduces MNIST with N examples for each digit class. In this way, RMNIST/1 has 1 training example for each digit, for a total of only 10 training examples.","tags":[],"title":"RMNIST/N: Train MNIST dataset with only TEN samples","type":"research"},{"authors":["Kan Huang","Kai Zhang","Ming Liu"],"categories":null,"content":"","date":1656633600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1656633600,"objectID":"b505a5e2b71f5619b8e2b57d9b666cba","permalink":"https://AI-Huang.github.io/publication/amlts2022/","publishdate":"2022-01-01T00:00:00Z","relpermalink":"/publication/amlts2022/","section":"publication","summary":"Accompanying rapid industrialization, humans are suffering from serious air pollution problems. The demand for air quality prediction is becoming more and more important to the government’s policy-making and people’s daily life. In this paper, We propose GreenEyes – a deep neural network model, which consists of a WaveNet-based backbone block for learning representations of sequences and an LSTM with a Temporal Attention module for capturing the hidden interactions between features of multi-channel inputs. To evaluate the effectiveness of our proposed method, we carry out several experiments including an ablation study on our collected and preprocessed air quality data near HKUST. The experimental results show our model can effectively predict the air quality level of the next timestamp given any segment of the air quality data from the data set.","tags":["machine learning"],"title":"GreenEyes: An Air Quality Evaluating Model based on WaveNet","type":"publication"},{"authors":[],"categories":[],"content":"Greedy Technology is a company based in Beijing. It focuses on teaching students machine learning knowledge and related technical applications, with a professional educating team. I assisted this company as a Course Lecturer during Oct. 2021 to Apr. 2022, and designed one basic tutorial and five application courses.\nTutorial:\nChatbot based on RNN and PyTorch Application Courses:\nObject Detection based on Faster RCNN Image Style Transfer based on Neural Style Image Captioning based on RNN, Code Earthquake Prediction based on WaveNet, Code Logo Synthesis based on iWGAN ","date":1631514180,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1631514180,"objectID":"b0dcc97d2939a88b734d4dd3a64108f7","permalink":"https://AI-Huang.github.io/teaching/course_lecturer/","publishdate":"2021-09-13T14:23:00+08:00","relpermalink":"/teaching/course_lecturer/","section":"teaching","summary":"Oct. 2021 ‐ Apr. 2022, Kowloon, Hong Kong. I assisted this company as a Course Lecturer and designed tutorials and courses.","tags":[],"title":"Course Lecturer, Greedy Technology","type":"teaching"},{"authors":[],"categories":[],"content":" Found new approaches of using flow models to do compression tasks for image‐like data. Wrote processing and feature extracting functions for 80K+ target data. Tested VQ‐VAE and basic Real NVP model to build unsupervising learning baselines; Based on open source codes, wrote hundreds lines of extra codes to restore Real NVP Compression model introduced by the SOTA papers. Did experiments with different feature extractors on the 80K+ 2D data; it turned out that the permutations of pixels will influence the results; Found Glow model’s learning potential; transplanted it from TensorFlow to PyTorch; Conclusion: flow models have great potential on distribution transformation, but better permutated data is also necessary. ","date":1621328280,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1621328280,"objectID":"ffa91a157da1824a0068619301b91e74","permalink":"https://AI-Huang.github.io/research/flow_model/","publishdate":"2021-05-18T16:58:00+08:00","relpermalink":"/research/flow_model/","section":"research","summary":"Found new approaches of using flow models to do compression tasks for image‐like data. Wrote processing and feature extracting functions for 80K+ target data. Tested VQ‐VAE and basic Real NVP model to build unsupervising learning baselines; Based on open source codes, wrote hundreds lines of extra codes to restore Real NVP Compression model introduced by the SOTA papers.","tags":[],"title":"Image Compression with Flow Models","type":"research"},{"authors":["Kan Huang"],"categories":["Codes","Networks"],"content":"This is a A TensorFlow Implementation of AdderNet. The original paper, AdderNet (H. Chen, et al., AdderNet: Do We Really Need Multiplications in Deep Learning?), is implemented with PyTorch. Here we provide a TensorFlow alternative for training and evaluating the Adder layer operator and related network backbones.\nThe codes is available (click the “Codes” button).\n","date":1611910800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1611910800,"objectID":"4ffce5dfc27af7766d166b2f04da2b66","permalink":"https://AI-Huang.github.io/research/addernet_tensorflow/","publishdate":"2021-01-29T17:00:00+08:00","relpermalink":"/research/addernet_tensorflow/","section":"research","summary":"This is a A TensorFlow Implementation of AdderNet. The original paper, AdderNet (H. Chen, et al., AdderNet: Do We Really Need Multiplications in Deep Learning?), is implemented with PyTorch. Here we provide a TensorFlow alternative for training and evaluating the Adder layer operator and related network backbones.","tags":["Replication","Computer Vision","TensorFlow","AdderNet"],"title":"A TensorFlow Implementation of AdderNet","type":"research"},{"authors":["Kan Huang"],"categories":["Reading Group"],"content":"This is a paper digest sharing presentation in the reading group. The featured paper is AdderNet (H. Chen, et al., AdderNet: Do We Really Need Multiplications in Deep Learning?).\nPresentation slides is available (click the “Slides” button).\n","date":1611910800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1611910800,"objectID":"d82694a7cec734ead221bb247dc76dad","permalink":"https://AI-Huang.github.io/research/addernet_presentation/","publishdate":"2021-01-29T17:00:00+08:00","relpermalink":"/research/addernet_presentation/","section":"research","summary":"This is a paper digest sharing presentation in the reading group. The featured paper is AdderNet (H. Chen, et al., AdderNet: Do We Really Need Multiplications in Deep Learning?).\nPresentation slides is available (click the “Slides” button).","tags":["Replication","Computer Vision"],"title":"AdderNet Presentation","type":"research"},{"authors":[],"categories":[],"content":" Helped Prof. YAO Yuan and the Nexperia company with the Nexperia semi‐conductor classification problem; Distinguished broken chips from normal chips by using deep learning and image classification methods; Compared the difference of perform between different models, e.g., ResNet20, ResNet56; Tried transferred learning methods. It turned out that a ResNet56 model transferred from a pretrained ResNet20 model could perform better, than simply training a ResNet56 model directly. ","date":1580358300,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580358300,"objectID":"c1d0de774efc47758907ccc2a373cd2d","permalink":"https://AI-Huang.github.io/research/semi_conductor/","publishdate":"2020-01-30T12:25:00+08:00","relpermalink":"/research/semi_conductor/","section":"research","summary":"Helped Prof. YAO Yuan and the Nexperia company with the Nexperia semi‐conductor classification problem; Distinguished broken chips from normal chips by using deep learning and image classification methods; Compared the difference of perform between different models, e.","tags":[],"title":"Semi-conductor Image Classification","type":"research"},{"authors":[],"categories":[],"content":"This program:\nBuilds a Seq2Seq model and learn on the text dataset; Uses beam search algorithm to generate output; Supplys a dialogue robot backend for the Chatbot. Other parts of the Chatbot.\n","date":1559218800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559218800,"objectID":"34f98880b4399fcbc0781dc946b5567e","permalink":"https://AI-Huang.github.io/research/seq2seq_chatbot/","publishdate":"2019-05-30T20:20:00+08:00","relpermalink":"/research/seq2seq_chatbot/","section":"research","summary":"This program:\nBuilds a Seq2Seq model and learn on the text dataset; Uses beam search algorithm to generate output; Supplys a dialogue robot backend for the Chatbot. Other parts of the Chatbot.","tags":[],"title":"Seq2Seq Chatbot","type":"research"},{"authors":[],"categories":[],"content":"Many people are interested about start-ups on education industries in Hong Kong. During Apr. 2019 to May. 2019, I served as Course Designer to help several entrepreneurship founders design lectures and courses to introduce knowledge in the university to common high school students.\nI designed a lecture which introduces the prelimilary background of ECE. Download my PPT on Introduction to ECE.\nIn the meanwhile, I have designed several tiny embedded projects to enhance the understanding of the background of ECE:\nRunning Water Lights; Electronic Voter. ","date":1556196540,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1556196540,"objectID":"2a3ee2941ed0eb16c71c2374d99d1781","permalink":"https://AI-Huang.github.io/teaching/course_designer/","publishdate":"2019-04-25T20:49:00+08:00","relpermalink":"/teaching/course_designer/","section":"teaching","summary":"Apr. 2019 ‐ May. 2019, Clearwater Bay, Hong Kong. I served as Course Designer to help several entrepreneurship founders design lectures and courses to introduce knowledge in the university to common high school students.","tags":[],"title":"Course Designer","type":"teaching"},{"authors":[],"categories":[],"content":" Built a chatbot on the QQ platform with the Coolq Framework. Implemented the following functions chatbot: built an application interface for the Seq2Seq model, gave the chatbot the ability to response for users’ input; dictionary command: using Web crawler, the chatbot can return word’s meaning; translate command: using Google translate library, the chatbot can retrive the translation for users’ input; sticker command: the chatbot could query the SQL database to find stickers that users demand; Other features: group members’ same words repeater; weather querying; banning user’s chat; search image by image. The Chatbot’s dialogue robot model.\n","date":1550217720,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1550217720,"objectID":"3af9db7a35ed9f15af0ffad25e486310","permalink":"https://AI-Huang.github.io/projects/seq2seq_qq_chatbot/","publishdate":"2019-02-15T16:02:00+08:00","relpermalink":"/projects/seq2seq_qq_chatbot/","section":"projects","summary":"Built a chatbot on the QQ platform with the Coolq Framework. Implemented the following functions chatbot: built an application interface for the Seq2Seq model, gave the chatbot the ability to response for users’ input; dictionary command: using Web crawler, the chatbot can return word’s meaning; translate command: using Google translate library, the chatbot can retrive the translation for users’ input; sticker command: the chatbot could query the SQL database to find stickers that users demand; Other features: group members’ same words repeater; weather querying; banning user’s chat; search image by image.","tags":["software development","chatbot"],"title":"Seq2Seq QQ Chatbot","type":"projects"},{"authors":[],"categories":[],"content":"I have served as Teaching Assistant for one year and a half at HKUST. Courses I served as TA:\nELEC2600: Stochastic Processes ELEC6910R: Robotic Perception and Learning ELEC6910R is a very interesting course about deep learning and robotics taught by my supervisor Prof. Ming LIU.\nDownload the course materials here.\n","date":1485878400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1485878400,"objectID":"a353ea9c38d3907180b8f679323da199","permalink":"https://AI-Huang.github.io/teaching/teaching_assistant/","publishdate":"2017-02-01T00:00:00+08:00","relpermalink":"/teaching/teaching_assistant/","section":"teaching","summary":"Feb. 2017 ‐ Jun. 2018, HKUST, Clearwater Bay, Hong Kong. I have served as Teaching Assistant for one year and a half at HKUST.","tags":[],"title":"Teaching Assistant","type":"teaching"},{"authors":[],"categories":null,"content":" Seed Cup 2014 Champion ","date":1416314627,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1416314627,"objectID":"9d1cf0f3b0d3f98bf35f3978904f8e27","permalink":"https://AI-Huang.github.io/events/seedcup2014/","publishdate":"2014-11-18T20:43:47+08:00","relpermalink":"/events/seedcup2014/","section":"events","summary":" Seed Cup 2014 Champion ","tags":["programming"],"title":"Seed Cup 2014","type":"events"}]