<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Computer Vision | Academic</title><link>https://AI-Huang.github.io/tag/computer-vision/</link><atom:link href="https://AI-Huang.github.io/tag/computer-vision/index.xml" rel="self" type="application/rss+xml"/><description>Computer Vision</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Thu, 01 Dec 2022 17:28:00 +0800</lastBuildDate><image><url>https://AI-Huang.github.io/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png</url><title>Computer Vision</title><link>https://AI-Huang.github.io/tag/computer-vision/</link></image><item><title>Replication of SENet</title><link>https://AI-Huang.github.io/research/replication_senet/</link><pubDate>Thu, 01 Dec 2022 17:28:00 +0800</pubDate><guid>https://AI-Huang.github.io/research/replication_senet/</guid><description>&lt;p>This is a replication of the work SENet (&lt;a href="https://arxiv.org/abs/1709.01507" target="_blank" rel="noopener">J. Hu, et al., Squeeze-and-Excitation Networks&lt;/a>). My codes:&lt;/p>
&lt;ul>
&lt;li>Implement the SENet module;&lt;/li>
&lt;li>Apply the SENet module to the ResNet;&lt;/li>
&lt;li>Train the ResNet with SENet on CIFAR-10;&lt;/li>
&lt;li>Use the re-trained benchmark results of ResNet on CIFAR-10 for comparative evaluation.&lt;/li>
&lt;/ul>
&lt;p>For statistical validation, each group of experiment has been run for 5 times.&lt;/p>
&lt;p>J. Hu et al.&amp;rsquo;s experiment group uses pad, crop and flip augmentation, while I use random tranlation augmentation. Both groups use standard deviation normalization. Number in the brackets of test accuracy are the difference from the ResNet backbone to the SE-ResNet counterpart.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model&lt;/th>
&lt;th>Author&lt;/th>
&lt;th>best test accuracy&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>ResNet20v1_CIFAR10&lt;/td>
&lt;td>Kan&lt;/td>
&lt;td>91.30%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ResNet32v1_CIFAR10&lt;/td>
&lt;td>Kan&lt;/td>
&lt;td>92.16%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ResNet110v1_CIFAR10&lt;/td>
&lt;td>Kan&lt;/td>
&lt;td>92.10%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ResNet164v1_CIFAR10&lt;/td>
&lt;td>Kan&lt;/td>
&lt;td>91.74%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>SE-ResNet20 (γ=16)&lt;/td>
&lt;td>Kan&lt;/td>
&lt;td>91.70% (+0.4)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>SE-ResNet32 (γ=16)&lt;/td>
&lt;td>Kan&lt;/td>
&lt;td>92.44% (+0.28)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>SE-ResNet110 (γ=16)&lt;/td>
&lt;td>Kan&lt;/td>
&lt;td>86.56% (-5.54)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>SE-ResNet164 (γ=16)&lt;/td>
&lt;td>Kan&lt;/td>
&lt;td>55.25% (-36.49)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>SE-ResNet110 (γ=16)&lt;/td>
&lt;td>J. Hu et al.&lt;/td>
&lt;td>5.21 (94.79%) (+1.16)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>SE-ResNet164 (γ=16)&lt;/td>
&lt;td>J. Hu et al.&lt;/td>
&lt;td>4.39 (95.61%) (+1.07)&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>Experiments on SE-ResNet20 and SE-ResNet32 show that SE module works well on enhancing the backbone network. But due to the backbone&amp;rsquo;s performance limitation (e.g., ResNet20), such enhancement is relatively limited. Training on SE-ResNet110 and SE-ResNet164 doesn&amp;rsquo;t converge, I&amp;rsquo;m still figuring out why.&lt;/p></description></item><item><title>Benchmark of ResNet on CIFAR-10</title><link>https://AI-Huang.github.io/research/benchmark_resnet/</link><pubDate>Tue, 29 Nov 2022 13:19:00 +0800</pubDate><guid>https://AI-Huang.github.io/research/benchmark_resnet/</guid><description>&lt;p>This is a &lt;strong>TensorFlow&lt;/strong> replication of experiments on CIFAR-10 mentioned in ResNet (&lt;a href="https://arxiv.org/abs/1512.03385" target="_blank" rel="noopener">K. He, et al., Deep Residual Learning for Image Recognition&lt;/a>). My codes:&lt;/p>
&lt;ul>
&lt;li>Adapts Keras&amp;rsquo;s example codes of ResNet for CIFAR-10 (note that this is a simpler version specially designed for CIFAR-10);&lt;/li>
&lt;li>Apply the SENet module to the ResNet;&lt;/li>
&lt;li>Re-train the ResNet w/o SENet on CIFAR-10 for benchmark evaluation.&lt;/li>
&lt;/ul>
&lt;p>For statistical validation, each group of experiment has been run for 5 times.&lt;/p>
&lt;p>Firstly, I try to reproduce the results with totally the same &lt;a href="https://github.com/AI-Huang/CV_Playground/blob/master/models/tf_fn/resnet_cifar10.py" target="_blank" rel="noopener">model codes&lt;/a> given by Keras. The batch size is set to 32. The optimizer is Adam with an initial learning rate 0.001. The validation split is 0, and the whole train set is used as training data. The data augmentation could be found at &lt;a href="https://github.com/AI-Huang/CV_Playground/blob/master/data_loaders/tf_fn/data_generator.py" target="_blank" rel="noopener">keras_augmentation&lt;/a>. The leanring scheduler could be found at &lt;a href="https://github.com/AI-Huang/CV_Playground/blob/master/models/tf_fn/optim_utils.py" target="_blank" rel="noopener">keras_lr_scheduler&lt;/a>. Our results outperform Keras&amp;rsquo;s on &lt;code>ResNet44v1_CIFAR10&lt;/code>, but go worse a little on other models.&lt;/p>
&lt;p>ResNet20 keras_augmentation Adam 0.001 0 keras_lr_scheduler 0.9183&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model&lt;/th>
&lt;th>Author&lt;/th>
&lt;th>best test accuracy&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>ResNet20v1_CIFAR10&lt;/td>
&lt;td>Keras&lt;/td>
&lt;td>0.9216&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ResNet20v1_CIFAR10&lt;/td>
&lt;td>Kan&lt;/td>
&lt;td>0.9183&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ResNet32v1_CIFAR10&lt;/td>
&lt;td>Keras&lt;/td>
&lt;td>0.9246&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ResNet32v1_CIFAR10&lt;/td>
&lt;td>Kan&lt;/td>
&lt;td>0.9227&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ResNet44v1_CIFAR10&lt;/td>
&lt;td>Keras&lt;/td>
&lt;td>0.9250&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ResNet44v1_CIFAR10&lt;/td>
&lt;td>Kan&lt;/td>
&lt;td>0.9252&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ResNet56v1_CIFAR10&lt;/td>
&lt;td>Keras&lt;/td>
&lt;td>0.9271&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ResNet56v1_CIFAR10&lt;/td>
&lt;td>Kan&lt;/td>
&lt;td>0.9236&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ResNet110v1_CIFAR10&lt;/td>
&lt;td>Keras&lt;/td>
&lt;td>0.9265&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ResNet110v1_CIFAR10&lt;/td>
&lt;td>Kan&lt;/td>
&lt;td>0.9260&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>Later, I follow the same configuration by K. He, et al., and use standard normalization. Random translation (padding, and then crop with a horizonal flip) is also applied. The validation split changes from 0 to 0.1 thus results in a 45k/5k train/val split. We follow the same optimizer settings. We use the SGD optimizer with an initial learning rate of 0.1, a momentum of 0.9, a weight decay of 0.0001 and finally a mini-batch size of 128.
&lt;code>cifar10_scheduler&lt;/code> is applied. In original paper, learning rate is reduced by a factor 0.1 on some a step such as the 32k and 48k step. Here we convert these step indices into epoch indices by equation:
$$ steps_per_epoch = \lceil \frac{45000}{batch_size} \rceil , $$
$$ epoch_to_reduce_lr = \lceil \frac{32000|48000}{steps_per_epoch} \rceil . $$
Thus we have an adapted schedule:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>steps (batchsize 128)&lt;/th>
&lt;th>epoch&lt;/th>
&lt;th>LR (SGD)&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>32k&lt;/td>
&lt;td>1~91&lt;/td>
&lt;td>0.1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>48k&lt;/td>
&lt;td>92~137&lt;/td>
&lt;td>0.01&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>64k&lt;/td>
&lt;td>137~182&lt;/td>
&lt;td>0.001&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>The results are listed as the table below:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Model&lt;/th>
&lt;th>pre-processing&lt;/th>
&lt;th>data_augmentation&lt;/th>
&lt;th>Author&lt;/th>
&lt;th>best test accuracy&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>ResNet20v1_CIFAR10&lt;/td>
&lt;td>subtract_mean&lt;/td>
&lt;td>pad_crop_flip&lt;/td>
&lt;td>He et al.&lt;/td>
&lt;td>91.25%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ResNet20v1_CIFAR10&lt;/td>
&lt;td>std_norm&lt;/td>
&lt;td>random_translation&lt;/td>
&lt;td>Kan&lt;/td>
&lt;td>91.30%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ResNet32v1_CIFAR10&lt;/td>
&lt;td>subtract_mean&lt;/td>
&lt;td>pad_crop_flip&lt;/td>
&lt;td>He et al.&lt;/td>
&lt;td>92.49%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ResNet32v1_CIFAR10&lt;/td>
&lt;td>std_norm&lt;/td>
&lt;td>random_translation&lt;/td>
&lt;td>Kan&lt;/td>
&lt;td>92.16%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ResNet44v1_CIFAR10&lt;/td>
&lt;td>subtract_mean&lt;/td>
&lt;td>pad_crop_flip&lt;/td>
&lt;td>He et al.&lt;/td>
&lt;td>92.83%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ResNet44v1_CIFAR10&lt;/td>
&lt;td>std_norm&lt;/td>
&lt;td>random_translation&lt;/td>
&lt;td>Kan&lt;/td>
&lt;td>N/A&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ResNet56v1_CIFAR10&lt;/td>
&lt;td>subtract_mean&lt;/td>
&lt;td>pad_crop_flip&lt;/td>
&lt;td>He et al.&lt;/td>
&lt;td>93.03%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ResNet56v1_CIFAR10&lt;/td>
&lt;td>std_norm&lt;/td>
&lt;td>random_translation&lt;/td>
&lt;td>Kan&lt;/td>
&lt;td>N/A&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ResNet110v1_CIFAR10&lt;/td>
&lt;td>subtract_mean&lt;/td>
&lt;td>pad_crop_flip&lt;/td>
&lt;td>He et al.&lt;/td>
&lt;td>93.39+-.16%&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ResNet110v1_CIFAR10&lt;/td>
&lt;td>std_norm&lt;/td>
&lt;td>random_translation&lt;/td>
&lt;td>Kan&lt;/td>
&lt;td>0.9210&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ResNet164v1_CIFAR10&lt;/td>
&lt;td>subtract_mean&lt;/td>
&lt;td>pad_crop_flip&lt;/td>
&lt;td>He et al.&lt;/td>
&lt;td>N/A&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ResNet164v1_CIFAR10&lt;/td>
&lt;td>std_norm&lt;/td>
&lt;td>random_translation&lt;/td>
&lt;td>Kan&lt;/td>
&lt;td>0.9174&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>The main difference between &lt;code>random_translation&lt;/code> and &lt;code>pad_crop_flip&lt;/code> is that the former supply with a new &lt;code>fill_mode='nearest'&lt;/code>.
Among the model that both He et al. and me have the results, our results only outperform He&amp;rsquo;s on ResNet20v1_CIFAR10, and are not as good as He claims on ResNet32v1_CIFAR10 and ResNet110v1_CIFAR10.
Models listed below are further used as benchmarks for SENet counterpart:&lt;/p>
&lt;ul>
&lt;li>ResNet20v1_CIFAR10&lt;/li>
&lt;li>ResNet32v1_CIFAR10&lt;/li>
&lt;li>ResNet110v1_CIFAR10&lt;/li>
&lt;li>ResNet164v1_CIFAR10&lt;/li>
&lt;/ul></description></item><item><title>A TensorFlow Implementation of AdderNet</title><link>https://AI-Huang.github.io/research/addernet_tensorflow/</link><pubDate>Fri, 29 Jan 2021 17:00:00 +0800</pubDate><guid>https://AI-Huang.github.io/research/addernet_tensorflow/</guid><description>&lt;p>This is a A TensorFlow Implementation of AdderNet. The original paper, AdderNet (&lt;a href="https://arxiv.org/abs/1912.13200" target="_blank" rel="noopener">H. Chen, et al., AdderNet: Do We Really Need Multiplications in Deep Learning?&lt;/a>), is implemented with PyTorch. Here we provide a TensorFlow alternative for training and evaluating the Adder layer operator and related network backbones.&lt;/p>
&lt;p>The codes is available (click the &amp;ldquo;Codes&amp;rdquo; button).&lt;/p></description></item><item><title>AdderNet Presentation</title><link>https://AI-Huang.github.io/research/addernet_presentation/</link><pubDate>Fri, 29 Jan 2021 17:00:00 +0800</pubDate><guid>https://AI-Huang.github.io/research/addernet_presentation/</guid><description>&lt;p>This is a paper digest sharing presentation in the reading group. The featured paper is AdderNet (&lt;a href="https://arxiv.org/abs/1912.13200" target="_blank" rel="noopener">H. Chen, et al., AdderNet: Do We Really Need Multiplications in Deep Learning?&lt;/a>).&lt;/p>
&lt;p>Presentation slides is available (click the &amp;ldquo;Slides&amp;rdquo; button).&lt;/p></description></item></channel></rss>